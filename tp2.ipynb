{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFdLVPxh9BlL"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pandas in c:\\users\\allag\\miniconda3\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\allag\\miniconda3\\lib\\site-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\allag\\miniconda3\\lib\\site-packages (3.9.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from matplotlib) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from matplotlib) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\allag\\miniconda3\\lib\\site-packages (1.6.0)\n",
            "Requirement already satisfied: seaborn in c:\\users\\allag\\miniconda3\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pandas>=1.2 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from seaborn) (2.2.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from seaborn) (3.9.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\allag\\miniconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas numpy matplotlib\n",
        "%pip install scikit-learn seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p1e5R3n39DVM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-LKiAmH9KBk"
      },
      "source": [
        "#### Fonction d'evaluation selon les m√©trics demande"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PewGBEB99OJk"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_val: np.ndarray, y_val: np.ndarray):\n",
        "    \"\"\"Evaluate the model using specified metrics.\"\"\"\n",
        "    predictions = model.predict(X_val)\n",
        "    probs = model.predict_proba(X_val)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "    metrics = {\n",
        "        \"Accuracy\": accuracy_score(y_val, predictions),\n",
        "        \"Precision\": precision_score(y_val, predictions, zero_division=0),\n",
        "        \"Recall\": recall_score(y_val, predictions, zero_division=0),\n",
        "        \"F1-score\": f1_score(y_val, predictions, zero_division=0),\n",
        "        \"ROC-AUC\": roc_auc_score(y_val, probs) if probs is not None else \"N/A\"\n",
        "    }\n",
        "\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value}\" if isinstance(value, float) else f\"{metric}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jisO7u3o9PuI"
      },
      "source": [
        "#### class Tik-Tak"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uU4b9yhZ9c-D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "class RandomForestOptimizer:\n",
        "    def __init__(self, param_ranges):\n",
        "        self.param_ranges = param_ranges\n",
        "\n",
        "    def initialize_model(self, params, param_ranges):\n",
        "        n_estimators = int(param_ranges['n_estimators'][0] + params[0] * (param_ranges['n_estimators'][1] - param_ranges['n_estimators'][0]))\n",
        "        max_depth = int(param_ranges['max_depth'][0] + params[1] * (param_ranges['max_depth'][1] - param_ranges['max_depth'][0]))\n",
        "        min_samples_split = int(param_ranges['min_samples_split'][0] + params[2] * (param_ranges['min_samples_split'][1] - param_ranges['min_samples_split'][0]))\n",
        "        min_samples_leaf = int(param_ranges['min_samples_leaf'][0] + params[3] * (param_ranges['min_samples_leaf'][1] - param_ranges['min_samples_leaf'][0]))\n",
        "\n",
        "        return RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            min_samples_split=min_samples_split,\n",
        "            min_samples_leaf=min_samples_leaf,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    def params_from_solution(self, solution: np.ndarray):\n",
        "        params = {}\n",
        "        for i, (param_name, (min_val, max_val)) in enumerate(self.param_ranges.items()):\n",
        "            params[param_name] = int(solution[i] * (max_val - min_val) + min_val)\n",
        "        return params\n",
        "\n",
        "    def objective_function(self, solution: np.ndarray, X_train, y_train, X_val, y_val):\n",
        "        model = RandomForestClassifier(**self.params_from_solution(solution))\n",
        "        model.fit(X_train, y_train)\n",
        "        predictions = model.predict(X_val)\n",
        "        return 1 - accuracy_score(y_val, predictions)\n",
        "\n",
        "    \n",
        "\n",
        "    def tiki_taka_optimization(self, population: np.ndarray, X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray, iterations: int = 10, exchange_rate: float = 0.1, exchange_intensity: float = 0.05):\n",
        "        print('Starting Tiki-Taka optimization...')\n",
        "        population = population.copy()\n",
        "\n",
        "        for _ in range(iterations):\n",
        "            # Exchanging information: Make slight modifications to solutions\n",
        "            for i in range(population.shape[0]):\n",
        "                current_solution = population[i].copy()\n",
        "\n",
        "                # Determine how many elements to exchange based on the exchange rate\n",
        "                num_exchanges = int(exchange_rate * population.shape[1])\n",
        "                exchange_indices = np.random.choice(population.shape[1], num_exchanges, replace=False)\n",
        "\n",
        "                # Make slight modifications at the chosen indices\n",
        "                for idx in exchange_indices:\n",
        "                    current_solution[idx] += np.random.uniform(-exchange_intensity, exchange_intensity)\n",
        "                    current_solution[idx] = np.clip(current_solution[idx], 0, 1)  # Ensure it remains within bounds\n",
        "\n",
        "                # Evaluate the fitness of the modified solution\n",
        "                current_fitness = self.objective_function(current_solution, X_train, y_train, X_val, y_val)\n",
        "                new_fitness = self.objective_function(population[i], X_train, y_train, X_val, y_val)\n",
        "\n",
        "                # Accept the new solution if it is better\n",
        "                if new_fitness < current_fitness:\n",
        "                    population[i] = current_solution\n",
        "\n",
        "        # Population Update: Rank solutions and keep the best\n",
        "        population = np.array(sorted(population, key=lambda x: self.objective_function(x, X_train, y_train, X_val, y_val)))\n",
        "        population = population[:population.shape[0]]  # Retain only the top solutions\n",
        "\n",
        "        return population\n",
        "\n",
        "    def optimize(self, population: np.ndarray, X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray, iterations: int = 10) -> np.ndarray:\n",
        "        \"\"\"Optimize the random forest parameters using Tiki-Taka optimization.\"\"\"\n",
        "         # Extract the parameters of the first individual in the population\n",
        "        initial_solution = population[0]\n",
        "        initial_params = self.params_from_solution(initial_solution)\n",
        "        initial_model = RandomForestClassifier(**initial_params)\n",
        "\n",
        "        # Print the initial parameters\n",
        "        print(\"\\nInitial Parameters for the First Individual:\")\n",
        "        for param, value in initial_params.items():\n",
        "            print(f\"{param}: {value}\")\n",
        "        # Apply Tiki-Taka to optimize the population\n",
        "        population = self.tiki_taka_optimization(population, X_train, y_train, X_val, y_val, iterations)\n",
        "\n",
        "        # Evaluate the best solution in the final population\n",
        "        best_solution = population[0]\n",
        "        best_params = self.params_from_solution(best_solution)\n",
        "        best_model = RandomForestClassifier(**best_params)\n",
        "        best_model.fit(X_train, y_train)\n",
        "\n",
        "        print(\"\\nOptimization Complete.\")\n",
        "        print(\"Best Parameters:\", best_params)\n",
        "        print(\"\\nEvaluation Metrics:\")\n",
        "        self.evaluate_model(best_model, X_val, y_val)\n",
        "\n",
        "        return population, best_model\n",
        "\n",
        "\n",
        "    def evaluate_model(self, model, X_val, y_val):\n",
        "        # Evaluate model performance on validation set\n",
        "        predictions = model.predict(X_val)\n",
        "        accuracy = accuracy_score(y_val, predictions)\n",
        "        print(f\"Accuracy: {accuracy}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9LEhs0E9vYa"
      },
      "source": [
        "#### main contenat le prepropcessing et l'appel des fonction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hC4kQz0n9zTg",
        "outputId": "ecfff047-d734-46ba-a34d-647ce620a5b9"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Load the data\n",
        "    file_path = 'Facebook Dataset 9_9_2019.xlsx'\n",
        "    data = pd.ExcelFile(file_path).parse('Sheet1')\n",
        "\n",
        "    # Data cleaning and preprocessing\n",
        "    data_cleaned = data.drop(columns=['Link'])\n",
        "\n",
        "    # Handle missing values\n",
        "    data_cleaned['Likes'].fillna(0, inplace=True)\n",
        "    data_cleaned['Groups'].fillna(0, inplace=True)\n",
        "\n",
        "    # Update to include new categorical columns\n",
        "    categorical_columns = ['Education', 'Work', 'Living Place', 'Relationship \\\\ Family', 'Intro.', 'CLASS']\n",
        "    numeric_columns = ['Likes', 'Groups', 'Mutual Friends', 'Posts', 'Tags']\n",
        "\n",
        "    for col in categorical_columns:\n",
        "        data_cleaned[col].fillna('Unknown', inplace=True)\n",
        "\n",
        "    # Create processed dataset\n",
        "    data_processed = pd.get_dummies(data_cleaned, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # Convert remaining object columns to numeric\n",
        "    for col in data_processed.select_dtypes(include=['object']).columns:\n",
        "        data_processed[col] = pd.to_numeric(data_processed[col], errors='coerce')\n",
        "\n",
        "    # Convert boolean columns to int\n",
        "    data_processed = data_processed.astype({col: 'int' for col in data_processed.select_dtypes(include=['bool']).columns})\n",
        "\n",
        "    # Standardize numeric columns\n",
        "    scaler = StandardScaler()\n",
        "    data_processed[numeric_columns] = scaler.fit_transform(data_processed[numeric_columns])\n",
        "\n",
        "    # Prepare data for feature selection\n",
        "    data_clean = data_processed.dropna(subset=['CLASS_Real'])\n",
        "    X_clean = data_clean.drop('CLASS_Real', axis=1)\n",
        "    columns_to_drop = ['Profile Picture', 'CheckIn', 'Name_id']\n",
        "    X_clean = X_clean.drop(columns=[col for col in columns_to_drop if col in X_clean.columns])\n",
        "    y_clean = data_clean['CLASS_Real']\n",
        "\n",
        "    # Impute missing values\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_imputed = imputer.fit_transform(X_clean)\n",
        "\n",
        "    # Feature selection\n",
        "    selector = SelectKBest(score_func=f_classif, k=min(8, X_clean.shape[1]))\n",
        "    X_new = selector.fit_transform(X_imputed, y_clean)\n",
        "    selected_columns = X_clean.columns[selector.get_support()]\n",
        "    print(\"\\nSelected features:\", selected_columns)\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_new, y_clean, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameter ranges for optimization\n",
        "    param_ranges = {\n",
        "        'n_estimators': (50, 200),\n",
        "        'max_depth': (3, 20),\n",
        "        'min_samples_split': (2, 10),\n",
        "        'min_samples_leaf': (1, 5)\n",
        "    }\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = RandomForestOptimizer(param_ranges)\n",
        "\n",
        "    # Generate initial population\n",
        "    population_size = 100\n",
        "    population = np.random.random((population_size, len(param_ranges)))\n",
        "    # Run optimization and evaluate first iteration\n",
        "    first_model = optimizer.initialize_model(population[0], param_ranges)\n",
        "    first_model.fit(X_train, y_train)\n",
        "    print(\"\\nFirst Iteration Evaluation on Validation Set:\")\n",
        "    evaluate_model(first_model, X_val, y_val)\n",
        "\n",
        "    # Run optimization\n",
        "    final_population, best_model = optimizer.optimize(\n",
        "        population=population,\n",
        "        X_train=X_train,\n",
        "        y_train=y_train,\n",
        "        X_val=X_val,\n",
        "        y_val=y_val,\n",
        "        iterations=20\n",
        "    )\n",
        "\n",
        "    print(\"\\nFinal Evaluation on Test Set:\")\n",
        "    evaluate_model(best_model, X_test, y_test)\n",
        "\n",
        "    # Feature importance visualization\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': selected_columns,\n",
        "        'importance': best_model.feature_importances_\n",
        "    })\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.set(style=\"whitegrid\", font_scale=1.2)  # Setting the style of the plot and scale of the font\n",
        "    bar_plot = sns.barplot(data=feature_importance.sort_values('importance', ascending=False),\n",
        "                           x='importance', y='feature', palette='viridis')\n",
        "    \n",
        "    plt.title('Feature Importance in the Random Forest Model')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.ylabel('Features')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Annotate each bar with its value\n",
        "    for p in bar_plot.patches:\n",
        "        width = p.get_width()    # get bar length\n",
        "        plt.text(width,         # set the text at bar's right\n",
        "                 p.get_y() + p.get_height() / 2,  # y is position at center\n",
        "                 f'{width:.2%}',  # value of bar\n",
        "                 va='center')  # vertical alignment\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
